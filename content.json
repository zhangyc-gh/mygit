[{"title":"deeplearning学习笔记4 深度神经网络","slug":"deeplearning4","date":"2019-08-15T12:30:08.000Z","updated":"2019-08-15T14:47:31.403Z","comments":true,"path":"2019/08/15/deeplearning4/","link":"","permalink":"http://yoursite.com/2019/08/15/deeplearning4/","excerpt":"深度神经网络","text":"深度神经网络 L= 4\\\\ n^{[0]} = n_x = 3\\\\ n^{[1]} = 5\\\\ n^{[2]} = 5\\\\ n^{[3]} = 3\\\\ n^{[4]} = n^{[L]} = 1$\\color{brown}{向 量 化 前 向 传 播}$ Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}\\\\ A^{[l]} = g^{[l]}(Z^{[l]})目前只能用for循环完成L层传播。 使矩阵有正确维数 W^{[l]}:(n^{[l]},n^{[l-1]})\\\\ Z^{[l]},A^{[l]}:(n^{[l]},m)\\\\ b^{[l]}:(n^{[l]},1)【广播】为什么深度网络神经有效例：人脸识别的特征提取： 第一层：边缘检测； 第二层：组合边缘，组成眼睛等； 在之后：局部特征组合，成为人脸。 正向传播与反向传播正： 输入：$a^{[l-1]}$ 输出：$a^{[l]}$ 缓存：$z^{[l]} , W^{[l]} , b^{[l]}$ 反： 输入：$da^{[l]}$ 输出：$da^{[l-1]} , dW^{[l]} , db^{[l]}$ $\\color{brown}{表达式：}$ \\left\\{ \\begin{array}{} dZ^{[l]} = dA^{[l]}*g^{[l]'}(Z^{[l]})\\\\ dW^{[l]} = \\frac 1m dZ^{[l]}A^{[l-1]^T}\\\\ db^{[l]} = \\frac 1m np.sum(dZ^{[l]},axis=1,keepdim=True)\\\\ dA^{[l-1]} = W^{[l]^T}dZ^{[l]} \\end{array} \\right.可推出：$dZ^{[l]} = W^{[l+1]^T}dZ^{[l+1]} * g^{[l]’}(Z^{[l]})$ 参数与超参数参数：$W^{[l]} , b^{[l]}$ 超参数：$学习率\\alpha，迭代次数N，层数L，神经元个数n^{[l]}，激活函数g(z)。$决定了参数$W^{[l]}，b^{[l]}$的值。 超参数的选择：1.基于实践；2.cost function最小。","categories":[],"tags":[{"name":"19暑假深度学习","slug":"19暑假深度学习","permalink":"http://yoursite.com/tags/19暑假深度学习/"}]},{"title":"deeplearning学习笔记3 浅层神经网络","slug":"deeplearning3","date":"2019-08-13T08:32:04.000Z","updated":"2019-08-15T08:06:41.665Z","comments":true,"path":"2019/08/13/deeplearning3/","link":"","permalink":"http://yoursite.com/2019/08/13/deeplearning3/","excerpt":"课堂笔记准备知识","text":"课堂笔记准备知识 $z^{[1]}_i = W^{[1]}_ia^{[0]} + b^{[1]}_i$ (4,1) = (4,3) (3,1) + (4,1) $a^{[1]}_i = \\sigma(z^{[1]}_i)$ (4,1) = (4,1) $z^{[2]}_i= W^{[2]}_ia^{[1]}_i + b^{[2]}_i$ (1,1) = (1,4)(4,1) + (1,1) $a^{[2]}_i = \\sigma(z^{[2]}_i) $ (1,1) = (1,1) $\\color{brown}{“a”代表激活}$ A^{[1]} = \\begin{bmatrix} | & | & | & ... & | \\\\ a^{[1](1)} & a^{[1](2)} & a^{[1](3)} & ... & a^{[1](m)} \\\\ | & | & | & ... & | \\\\ \\end{bmatrix}横向：不同训练实例；纵向：不同隐藏单元节点 $\\color{brown}{向 量 化 代 码 实 现}$ Z^{[1]} = W^{[1]}X + b^{[1]} \\\\ A^{[1]} = \\sigma(Z^{[1]}) \\\\ Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]} \\\\ A^{[2]} = \\sigma(Z^{[2]}) \\\\选择激活函数 一般来说，tanh比sigmoid函数更优越。但是输出层还是要用sigmoid函数。因为输出需要是在0和1之间的数字。 隐藏层的激活函数不能是线性的。 反向传播的求导 dZ^{[2]} = A^{[2]} - Y \\\\ dW^{[2]} = \\frac 1mdZ^{[2]}A^{[1]^T} \\\\ db^{[2]} = \\frac 1mnp.sum(dZ^{[2]}.axis=1,keepdims = True) \\\\ dZ^{[1]} = W^{[2]^T}dZ^{[2]} * g^{[1]'}(Z^{[1]})*代表逐个元素相乘 \\\\ dW^{[1]} = \\frac 1mdZ^{[1]}X^T \\\\ db^{[1]} = \\frac 1mnp.sum(dz^{[1]},axis = 1,keepdims =True)keepdims是为了保证矩阵形式，也可用reshape实现。 随机初始化为防止对称问题 W^{[1]} = np.random.randn((2,2)) * 0.01 \\\\ b^{[1]} = np.zeros((2,1)) \\\\ W^{[2]} = np.random.randn((1,2)) * 0.01 \\\\ b^{[2]} = 0b对称失效；避免梯度下降缓慢，初始化数值小。","categories":[],"tags":[{"name":"19暑假深度学习","slug":"19暑假深度学习","permalink":"http://yoursite.com/tags/19暑假深度学习/"}]},{"title":"deeplearning学习笔记2 神经网络基础知识","slug":"deeplearning2","date":"2019-08-11T11:19:12.000Z","updated":"2019-08-15T07:58:50.080Z","comments":true,"path":"2019/08/11/deeplearning2/","link":"","permalink":"http://yoursite.com/2019/08/11/deeplearning2/","excerpt":"课堂笔记作为神经网络的逻辑回归·二元分类算法：输入一张图片——判断——输出0/1","text":"课堂笔记作为神经网络的逻辑回归·二元分类算法：输入一张图片——判断——输出0/1 【计算机存储图像】x是一个维度n(x)=n✖n✖3的特征向量，y是标签0/1。 总共有m个训练样本 ·逻辑回归z=yhat=p(y=1|x)=sigmoid(Wt*X)+b（W也是一个n(x)维向量） sigmoid(z)=1/(1+e^(-z)) ·损失函数梯度下降可以解决：凸像最优问题 L(yhat,y)=-(ylogyhat+(1-y)log(1-yhat)) 代价函数：J(w,b)=1/m sigma L(yhat,y) ·梯度下降对w进行迭代：w=w-αdw（α：learning rate） ·计算图正向传播及反向传播 ·导数 设计双重循环 12345678910111213for i=1 to m&#123;z(i)=WtX(i)+b;a(i)=sigma(z(i));J+=-[y(i)loga(i)+(1-y(i))log(1-a(i))];dz(i)=a(i)-y(i);&#123;for j=1 to ndw(j)+=xj(i)dz(i);&#125;db+=dz(i);&#125;J/=m;&#123;for j=1 to ndw(j)/=m;&#125;db/=m; Python和向量法·向量化运用Numpy内置函数 123456789import numpy as npimport timea= np.random.rand(1000000)b= np.random.rand(1000000)c = np.dot(a,b)for i in range(1000000): c +=a[i]*b[i] 向量化更快 避免使用显式for循环 123456u = np.zeros((n,1))import numpy as npu = np.exp(v)u = np.log(v)u = np.abs(v)u = np.maxnum (v,0) ·将双重循环省略12345678//for i in range(num): 迭代次数Z = np.dot(w.T,x)+bA = sigma(Z)dZ = A - Ydw = 1/m XdZtdb = 1/m np.sum(dZ)w = w - αdwb = b - αdb ·Python中的广播广播的规则: 让所有输入数组都向其中形状最长的数组看齐，形状中不足的部分都通过在前面加 1 补齐。 输出数组的形状是输入数组形状的各个维度上的最大值。 如果输入数组的某个维度和输出数组的对应维度的长度相同或者其长度为 1 时，这个数组能够用来计算，否则出错。 当输入数组的某个维度的长度为 1 时，沿着此维度运算时都用此维度上的第一组值。 简单理解：对两个数组，分别比较他们的每一个维度（若其中一个数组没有当前维度则忽略），满足： 数组拥有相同形状。 当前维度的值相等。 当前维度的值有一个是 1。 若条件不满足，抛出 “ValueError: frames are not aligned” 异常。 ·Python防bug指南不要使用秩为1的数组，始终使用n乘1的矩阵； 不要怕使用reshape操作，来确保矩阵和向量是需要的维度。 版权声明：累die，不声明","categories":[],"tags":[{"name":"19暑假深度学习","slug":"19暑假深度学习","permalink":"http://yoursite.com/tags/19暑假深度学习/"}]},{"title":"deeplearning学习笔记1 引言","slug":"deeplearning1","date":"2019-08-09T13:46:53.000Z","updated":"2019-08-15T12:23:27.538Z","comments":true,"path":"2019/08/09/deeplearning1/","link":"","permalink":"http://yoursite.com/2019/08/09/deeplearning1/","excerpt":"课堂笔记 ReLU（线性整流）函数【房价预测】","text":"课堂笔记 ReLU（线性整流）函数【房价预测】 ​ X-神经元的输入；计算线性方程；结果取max[0,y]。有时，X有多个。建立映射？ 监督学习(Supervised Learning) ​ 把输入x和输入y相对应起来。 结构化数据与非结构化数据 ​ Structured data: 表格数据 ​ Unstuctured data: 声/影/文 常见神经网络 ​ Standard &amp;&amp; CNN &amp;&amp; RNN ​ 用途 近年深度学习兴起原因 Scale has been driving deeplearning progress. 神经网络大 数据规模大 三因素 Data：暴增 Computation：GPU &amp;&amp; CPU Algorithms：Sigmoid to ReLU（梯度慢慢为0 到 梯度一直为1） 对 Geoffrey Hiton 的采访 ​ AI研究方向凭直觉系列 ​ 即使为之奋斗一生的方向错了，也总比抱有遗憾好。相信直觉，终身思考。 版权声明：版主原创，随便引用。","categories":[],"tags":[{"name":"19暑假深度学习","slug":"19暑假深度学习","permalink":"http://yoursite.com/tags/19暑假深度学习/"}]},{"title":"喵辉部长是男神","slug":"huihuitql","date":"2019-08-08T14:07:13.000Z","updated":"2019-08-08T14:13:57.296Z","comments":true,"path":"2019/08/08/huihuitql/","link":"","permalink":"http://yoursite.com/2019/08/08/huihuitql/","excerpt":"","text":"怀疑自己19号无法过科二的我在今天又享受了解决无数个沙雕问题的经历，不过博客网址加载出来的一刹那也是自己把自己满足了","categories":[],"tags":[{"name":"上辈子是道数学题系列","slug":"上辈子是道数学题系列","permalink":"http://yoursite.com/tags/上辈子是道数学题系列/"}]}]